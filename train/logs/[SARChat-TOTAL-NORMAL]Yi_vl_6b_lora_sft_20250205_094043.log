run sh: `/root/.conda/envs/swift-new-4/bin/python -m torch.distributed.run --nproc_per_node 2 /root/.conda/envs/swift-new-4/lib/python3.10/site-packages/swift/cli/sft.py --model /root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B --train_type lora --dataset /root/code/sarchat-exp/data/SARChat_total_task_train_modified.json --torch_dtype bfloat16 --num_train_epochs 1 --per_device_train_batch_size 4 --learning_rate 1e-4 --warmup_ratio 0.1 --report_to wandb --lora_rank 8 --lora_alpha 32 --target_modules all-linear --gradient_accumulation_steps 4 --save_steps 10000 --save_total_limit 5 --gradient_checkpointing_kwargs {"use_reentrant": false} --logging_steps 5 --max_length 2048 --output_dir ./swift_output/SARChat-Yi-VL-6B-Lora --system You are a helpful assistant. --dataloader_num_workers 16 --model_author JimmyMa99 --model_name SARChat-Yi-VL-6B-Lora --resume_from_checkpoint /root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v3-20250204-030621/checkpoint-10000`
[INFO:swift] Successfully registered `/root/.conda/envs/swift-new-4/lib/python3.10/site-packages/swift/llm/dataset/data/dataset_info.json`
[INFO:swift] Loading the model using model_dir: /root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v3-20250204-030621/checkpoint-10000
[INFO:swift] Successfully loaded /root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v3-20250204-030621/checkpoint-10000/args.json.
[INFO:swift] rank: 0, local_rank: 0, world_size: 2, local_world_size: 2
[INFO:swift] Loading the model using model_dir: /root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B
[INFO:swift] freeze_parameters: ['model.vision_tower', 'model.mm_projector']
[INFO:swift] Setting args.lazy_tokenize: True
/root/.conda/envs/swift-new-4/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/.conda/envs/swift-new-4/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[INFO:swift] output_dir: ./swift_output/SARChat-Yi-VL-6B-Lora/v5-20250205-094115
[INFO:swift] Global seed set to 42
[INFO:swift] args: TrainArguments(
_n_gpu=-1,
acc_steps=1,
acc_strategy=token,
accelerator_config={'dispatch_batches': False},
adafactor=False,
adalora_beta1=0.85,
adalora_beta2=0.85,
adalora_deltaT=1,
adalora_init_r=12,
adalora_orth_reg_weight=0.5,
adalora_target_r=8,
adalora_tfinal=0,
adalora_tinit=0,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
adapter_act=gelu,
adapter_length=128,
adapters=['/root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v3-20250204-030621/checkpoint-10000'],
add_version=True,
attn_impl=None,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_storage=None,
bnb_4bit_quant_type=nf4,
bnb_4bit_use_double_quant=True,
boft_block_num=0,
boft_block_size=4,
boft_dropout=0.0,
boft_n_butterfly_factor=1,
check_model=True,
ckpt_dir=/root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v3-20250204-030621/checkpoint-10000,
custom_dataset_info=[],
custom_register_path=[],
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=16,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset=['/root/code/sarchat-exp/data/SARChat_total_task_train_modified.json'],
dataset_num_proc=1,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=None,
deepspeed=None,
device_map=None,
disable_tqdm=None,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
download_mode=reuse_dataset_if_exists,
enable_cache=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=10000.0,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fourier_n_frequency=2000,
fourier_scaling=300.0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_aligner=True,
freeze_llm=False,
freeze_parameters=['model.vision_tower', 'model.mm_projector'],
freeze_parameters_ratio=0.0,
freeze_vit=True,
fsdp=,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_num=1,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
galore_cos_threshold=0.4,
galore_gamma_proj=2,
galore_optim_per_parameter=False,
galore_proj_bits=4,
galore_proj_group_size=256,
galore_proj_quant=False,
galore_proj_type=std,
galore_quantization=False,
galore_queue_size=5,
galore_rank=128,
galore_scale=1.0,
galore_target_modules=None,
galore_update_proj_gap=50,
galore_with_embedding=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': False},
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hqq_axis=None,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_args_error=False,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
init_weights=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
lazy_tokenize=True,
learning_rate=0.0001,
length_column_name=length,
lisa_activated_layers=0,
lisa_step_interval=20,
llamapro_num_groups=None,
llamapro_num_new_blocks=4,
load_args=True,
load_best_model_at_end=False,
load_data_args=False,
load_dataset_config=None,
local_rank=0,
local_repo_path=None,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v5-20250205-094115/runs,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
logprobs=False,
lora_alpha=32,
lora_bias=none,
lora_dropout=0.05,
lora_dtype=None,
lora_ga_batch_size=2,
lora_ga_direction=ArB2r,
lora_ga_iters=2,
lora_ga_max_length=1024,
lora_ga_scale=stable,
lora_ga_stable_gamma=16,
lora_modules=[],
lora_rank=8,
lorap_lr_ratio=None,
loss_scale=default,
loss_type=None,
lr_scheduler_kwargs=None,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=64,
max_pixels=None,
max_steps=-1,
metric=None,
metric_for_best_model=loss,
metric_warmup_step=0,
model=/root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B,
model_author=['JimmyMa99'],
model_kwargs={},
model_layer_cls_name=None,
model_name=['SARChat-Yi-VL-6B-Lora'],
model_revision=None,
model_type=yi_vl,
modules_to_save=[],
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_beams=1,
num_labels=None,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
optimizer=None,
output_dir=/root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v5-20250205-094115,
overwrite_output_dir=False,
packing=False,
padding_side=right,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_bits=None,
quant_method=None,
ray_scope=last,
reft_args=None,
reft_intervention_type=LoreftIntervention,
reft_layer_key=None,
reft_layers=None,
reft_rank=4,
remove_unused_columns=False,
repetition_penalty=None,
report_to=['wandb'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=/root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v3-20250204-030621/checkpoint-10000,
resume_only_model=False,
rope_scaling=None,
run_name=None,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=10000.0,
save_strategy=steps,
save_total_limit=5,
seed=42,
sequence_parallel_size=1,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
split_dataset_ratio=0.01,
stop_words=[],
stream=False,
streaming=False,
strict=False,
system=You are a helpful assistant.,
target_modules=['all-linear'],
target_regex=None,
task_type=causal_lm,
temperature=0.0,
template=yi_vl,
template_backend=swift,
tf32=None,
tools_prompt=react_en,
top_k=None,
top_logprobs=None,
top_p=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_dtype=torch.bfloat16,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_type=lora,
trainable_parameters=[],
truncation_strategy=delete,
tuner_backend=peft,
use_chat_template=True,
use_cpu=False,
use_dora=False,
use_galore=False,
use_hf=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger=False,
use_liger_kernel=False,
use_mps_device=False,
use_rslora=False,
use_swift_lora=False,
val_dataset=[],
vera_d_initial=0.1,
vera_dropout=0.0,
vera_projection_prng_key=0,
vera_rank=256,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.1,
)
[INFO:swift] Loading the model using model_dir: /root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B
[INFO:swift] local_repo_path: /root/.cache/modelscope/hub/_github/Yi
[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  9.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.44s/it]
Some weights of the model checkpoint at /root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B were not used when initializing LlavaLlamaForCausalLM: {'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias'}
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  9.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.46s/it]
Some weights of the model checkpoint at /root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B were not used when initializing LlavaLlamaForCausalLM: {'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.27.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.30.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.28.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.31.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.29.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias'}
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO:swift] Please ignore the above warning.
[INFO:swift] Loading the parameters of vision_tower...
[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}
[INFO:swift] model_info: ModelInfo(model_type='yi_vl', model_dir='/root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B', torch_dtype=torch.bfloat16, max_model_len=4096, quant_method=None, quant_bits=None, config=LlavaConfig {
  "_name_or_path": "./yi-vl-6b",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "image_aspect_ratio": "pad",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "mm_hidden_size": 1280,
  "mm_projector_type": "mlp2x_gelu_Norm",
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "/root/models/01ai/Yi-VL-6B/01ai/Yi-VL-6B/vit/clip-vit-H-14-laion2B-s32B-b79K-yi-vl-6B-448",
  "model_type": "llava",
  "moe_layer": null,
  "moe_mode": "easy",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 5000000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.1",
  "tune_mm_mlp_adapter": false,
  "tune_vision_tower": false,
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 64000
}
, task_type='causal_lm', num_labels=None)
[INFO:swift] model.generation_config: GenerationConfig {
  "bos_token_id": 6,
  "eos_token_id": 7,
  "max_new_tokens": 64,
  "pad_token_id": 0
}

[INFO:swift] Automatically add gradient_checkpointing to <class 'torch.nn.modules.container.ModuleList'>.
[INFO:swift] default_system: You are a helpful assistant.
[INFO:swift] The TrainArguments will be saved in: /root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v5-20250205-094115/args.json
[INFO:swift] Start time of running main: 2025-02-05 09:42:58.829678
[INFO:swift] create tmp_dir: /root/.cache/modelscope/hub/tmp/hf_datasets-4tscajg9
Map:   0%|          | 0/1836912 [00:00<?, ? examples/s]Map:   0%|          | 0/1836912 [00:00<?, ? examples/s]Map:   0%|          | 5000/1836912 [00:00<00:44, 41280.65 examples/s]Map:   0%|          | 5000/1836912 [00:00<00:44, 41596.21 examples/s]Map:   1%|          | 11000/1836912 [00:00<00:38, 47873.04 examples/s]Map:   1%|          | 11000/1836912 [00:00<00:38, 47898.18 examples/s]Map:   1%|          | 17000/1836912 [00:00<00:35, 50569.72 examples/s]Map:   1%|          | 17000/1836912 [00:00<00:36, 50457.13 examples/s]Map:   1%|▏         | 23000/1836912 [00:00<00:34, 51969.73 examples/s]Map:   1%|▏         | 24000/1836912 [00:00<00:39, 45370.49 examples/s]Map:   2%|▏         | 29000/1836912 [00:00<00:34, 52745.16 examples/s]Map:   2%|▏         | 30000/1836912 [00:00<00:38, 46926.37 examples/s]Map:   2%|▏         | 35000/1836912 [00:00<00:34, 51831.76 examples/s]Map:   2%|▏         | 36000/1836912 [00:00<00:36, 48986.52 examples/s]Map:   2%|▏         | 41000/1836912 [00:00<00:34, 52114.36 examples/s]Map:   2%|▏         | 42000/1836912 [00:00<00:35, 50196.15 examples/s]Map:   3%|▎         | 47000/1836912 [00:00<00:33, 52823.28 examples/s]Map:   3%|▎         | 48000/1836912 [00:00<00:34, 51240.33 examples/s]Map:   3%|▎         | 53000/1836912 [00:01<00:33, 53198.25 examples/s]Map:   3%|▎         | 54000/1836912 [00:01<00:34, 51636.05 examples/s]Map:   3%|▎         | 59000/1836912 [00:01<00:33, 53391.07 examples/s]Map:   3%|▎         | 60000/1836912 [00:01<00:34, 52137.22 examples/s]Map:   4%|▎         | 65000/1836912 [00:01<00:33, 53575.04 examples/s]Map:   4%|▎         | 66000/1836912 [00:01<00:33, 52519.77 examples/s]Map:   4%|▍         | 71000/1836912 [00:01<00:32, 53762.70 examples/s]Map:   4%|▍         | 72000/1836912 [00:01<00:33, 52853.76 examples/s]Map:   4%|▍         | 77000/1836912 [00:01<00:32, 53905.23 examples/s]Map:   4%|▍         | 78000/1836912 [00:01<00:33, 52635.90 examples/s]Map:   5%|▍         | 83000/1836912 [00:01<00:32, 54018.60 examples/s]Map:   5%|▍         | 84000/1836912 [00:01<00:33, 52782.69 examples/s]Map:   5%|▍         | 89000/1836912 [00:01<00:32, 54108.19 examples/s]Map:   5%|▍         | 90000/1836912 [00:01<00:33, 52833.02 examples/s]Map:   5%|▌         | 95000/1836912 [00:01<00:32, 54155.55 examples/s]Map:   5%|▌         | 96000/1836912 [00:01<00:32, 52967.80 examples/s]Map:   5%|▌         | 101000/1836912 [00:01<00:32, 54133.14 examples/s]Map:   6%|▌         | 107000/1836912 [00:02<00:31, 54163.02 examples/s]Map:   6%|▌         | 113000/1836912 [00:02<00:31, 54077.34 examples/s]Map:   6%|▌         | 102000/1836912 [00:02<00:48, 35881.39 examples/s]Map:   6%|▋         | 119000/1836912 [00:02<00:31, 53950.23 examples/s]Map:   6%|▌         | 108000/1836912 [00:02<00:43, 39815.95 examples/s]Map:   7%|▋         | 125000/1836912 [00:02<00:31, 53989.34 examples/s]Map:   6%|▌         | 114000/1836912 [00:02<00:39, 43150.76 examples/s]Map:   7%|▋         | 131000/1836912 [00:02<00:31, 53923.23 examples/s]Map:   7%|▋         | 120000/1836912 [00:02<00:37, 45709.80 examples/s]Map:   7%|▋         | 137000/1836912 [00:02<00:31, 53927.57 examples/s]Map:   7%|▋         | 126000/1836912 [00:02<00:35, 47807.55 examples/s]Map:   8%|▊         | 143000/1836912 [00:02<00:31, 54035.27 examples/s]Map:   7%|▋         | 132000/1836912 [00:02<00:34, 49132.09 examples/s]Map:   8%|▊         | 149000/1836912 [00:02<00:31, 54090.38 examples/s]Map:   8%|▊         | 138000/1836912 [00:02<00:33, 50417.35 examples/s]Map:   8%|▊         | 144000/1836912 [00:02<00:32, 51327.86 examples/s]Map:   8%|▊         | 150000/1836912 [00:03<00:32, 51834.77 examples/s]Map:   8%|▊         | 156000/1836912 [00:03<00:44, 37361.00 examples/s]Map:   8%|▊         | 156000/1836912 [00:03<00:32, 52388.66 examples/s]Map:   9%|▉         | 162000/1836912 [00:03<00:40, 41060.09 examples/s]Map:   9%|▉         | 162000/1836912 [00:03<00:31, 52712.11 examples/s]Map:   9%|▉         | 168000/1836912 [00:03<00:37, 44230.10 examples/s]Map:   9%|▉         | 168000/1836912 [00:03<00:31, 53079.42 examples/s]Map:   9%|▉         | 174000/1836912 [00:03<00:35, 46796.11 examples/s]Map:   9%|▉         | 174000/1836912 [00:03<00:31, 53142.89 examples/s]Map:  10%|▉         | 180000/1836912 [00:03<00:34, 48599.51 examples/s]Map:  10%|▉         | 180000/1836912 [00:03<00:31, 53193.85 examples/s]Map:  10%|█         | 186000/1836912 [00:03<00:33, 49979.56 examples/s]Map:  10%|█         | 186000/1836912 [00:03<00:30, 53302.50 examples/s]Map:  10%|█         | 192000/1836912 [00:03<00:32, 51189.43 examples/s]Map:  10%|█         | 192000/1836912 [00:03<00:30, 53240.39 examples/s]Map:  11%|█         | 198000/1836912 [00:03<00:31, 51949.49 examples/s]Map:  11%|█         | 198000/1836912 [00:03<00:30, 53331.37 examples/s]Map:  11%|█         | 204000/1836912 [00:03<00:31, 52296.57 examples/s]Map:  11%|█▏        | 210000/1836912 [00:04<00:30, 52640.91 examples/s]Map:  12%|█▏        | 216000/1836912 [00:04<00:30, 52689.97 examples/s]Map:  11%|█         | 204000/1836912 [00:04<00:46, 35053.28 examples/s]Map:  12%|█▏        | 222000/1836912 [00:04<00:30, 53060.00 examples/s]Map:  11%|█▏        | 210000/1836912 [00:04<00:41, 39093.08 examples/s]Map:  12%|█▏        | 228000/1836912 [00:04<00:30, 53358.89 examples/s]Map:  12%|█▏        | 216000/1836912 [00:04<00:38, 42404.87 examples/s]Map:  13%|█▎        | 236000/1836912 [00:04<00:31, 51321.90 examples/s]Map:  12%|█▏        | 222000/1836912 [00:04<00:35, 45237.69 examples/s]Map:  13%|█▎        | 242000/1836912 [00:04<00:30, 52083.94 examples/s]Map:  12%|█▏        | 229000/1836912 [00:04<00:36, 43692.72 examples/s]Map:  14%|█▎        | 250000/1836912 [00:04<00:31, 50350.24 examples/s]Map:  13%|█▎        | 234000/1836912 [00:04<00:36, 43705.11 examples/s]Map:  14%|█▍        | 256000/1836912 [00:04<00:30, 51255.19 examples/s]Map:  13%|█▎        | 241000/1836912 [00:05<00:38, 41664.92 examples/s]Map:  14%|█▍        | 262000/1836912 [00:05<00:30, 52031.34 examples/s]Map:  13%|█▎        | 247000/1836912 [00:05<00:35, 44540.32 examples/s]Map:  15%|█▍        | 268000/1836912 [00:05<00:29, 52566.03 examples/s]Map:  14%|█▍        | 253000/1836912 [00:05<00:33, 46891.72 examples/s]Map:  15%|█▍        | 274000/1836912 [00:05<00:29, 52872.00 examples/s]Map:  14%|█▍        | 259000/1836912 [00:05<00:32, 48465.86 examples/s]Map:  15%|█▌        | 280000/1836912 [00:05<00:29, 53217.71 examples/s]Map:  14%|█▍        | 265000/1836912 [00:05<00:31, 49862.78 examples/s]Map:  15%|█▍        | 271000/1836912 [00:05<00:30, 50929.41 examples/s]Map:  16%|█▌        | 288000/1836912 [00:05<00:41, 37739.54 examples/s]Map:  15%|█▌        | 277000/1836912 [00:05<00:30, 51647.35 examples/s]Map:  16%|█▌        | 294000/1836912 [00:05<00:37, 41217.81 examples/s]Map:  15%|█▌        | 283000/1836912 [00:05<00:30, 51638.65 examples/s]Map:  16%|█▋        | 300000/1836912 [00:05<00:34, 44059.61 examples/s]Map:  16%|█▌        | 289000/1836912 [00:05<00:30, 51021.22 examples/s]Map:  17%|█▋        | 306000/1836912 [00:06<00:32, 46538.81 examples/s]Map:  16%|█▌        | 295000/1836912 [00:06<00:29, 51815.73 examples/s]Map:  17%|█▋        | 312000/1836912 [00:06<00:31, 48483.33 examples/s]Map:  17%|█▋        | 318000/1836912 [00:06<00:30, 49681.60 examples/s]Map:  16%|█▋        | 302000/1836912 [00:06<00:42, 36419.53 examples/s]Map:  18%|█▊        | 324000/1836912 [00:06<00:29, 50788.29 examples/s]Map:  17%|█▋        | 308000/1836912 [00:06<00:38, 40184.15 examples/s]Map:  18%|█▊        | 330000/1836912 [00:06<00:29, 51680.12 examples/s]Map:  17%|█▋        | 314000/1836912 [00:06<00:35, 43338.94 examples/s]Map:  18%|█▊        | 336000/1836912 [00:06<00:28, 52009.44 examples/s]Map:  17%|█▋        | 320000/1836912 [00:06<00:32, 45967.91 examples/s]Map:  19%|█▊        | 342000/1836912 [00:06<00:28, 52668.79 examples/s]Map:  18%|█▊        | 326000/1836912 [00:06<00:31, 47834.01 examples/s]Map:  19%|█▉        | 348000/1836912 [00:06<00:28, 53004.46 examples/s]Map:  18%|█▊        | 332000/1836912 [00:06<00:30, 49479.75 examples/s]Map:  19%|█▉        | 354000/1836912 [00:06<00:27, 53419.85 examples/s]Map:  18%|█▊        | 338000/1836912 [00:07<00:29, 50678.39 examples/s]Map:  20%|█▉        | 360000/1836912 [00:07<00:27, 53689.55 examples/s]Map:  19%|█▊        | 344000/1836912 [00:07<00:28, 51597.39 examples/s]Map:  20%|█▉        | 366000/1836912 [00:07<00:27, 53679.06 examples/s]Map:  19%|█▉        | 350000/1836912 [00:07<00:28, 51845.28 examples/s]Map:  20%|██        | 372000/1836912 [00:07<00:27, 53839.87 examples/s]Map:  19%|█▉        | 356000/1836912 [00:07<00:28, 52404.29 examples/s]Map:  21%|██        | 378000/1836912 [00:07<00:27, 53982.18 examples/s]Map:  20%|█▉        | 362000/1836912 [00:07<00:27, 52787.54 examples/s]Map:  20%|██        | 368000/1836912 [00:07<00:27, 53050.17 examples/s]Map:  21%|██        | 384000/1836912 [00:07<00:39, 36673.94 examples/s]Map:  20%|██        | 374000/1836912 [00:07<00:27, 53194.78 examples/s]Map:  21%|██        | 390000/1836912 [00:07<00:35, 40576.97 examples/s]Map:  21%|██        | 380000/1836912 [00:07<00:27, 53374.22 examples/s]Map:  22%|██▏       | 396000/1836912 [00:07<00:32, 43866.84 examples/s]Map:  21%|██        | 386000/1836912 [00:07<00:27, 53302.30 examples/s]Map:  22%|██▏       | 402000/1836912 [00:08<00:30, 46504.66 examples/s]Map:  21%|██▏       | 392000/1836912 [00:08<00:27, 53420.91 examples/s]Map:  22%|██▏       | 408000/1836912 [00:08<00:29, 48369.73 examples/s]Map:  22%|██▏       | 398000/1836912 [00:08<00:26, 53493.31 examples/s]Map:  23%|██▎       | 414000/1836912 [00:08<00:28, 49528.70 examples/s]Map:  22%|██▏       | 404000/1836912 [00:08<00:26, 53539.94 examples/s]Map:  23%|██▎       | 420000/1836912 [00:08<00:27, 50914.36 examples/s]Map:  22%|██▏       | 410000/1836912 [00:08<00:26, 53626.95 examples/s]Map:  23%|██▎       | 426000/1836912 [00:08<00:27, 51867.91 examples/s]Map:  23%|██▎       | 416000/1836912 [00:08<00:26, 53683.77 examples/s]Map:  24%|██▎       | 432000/1836912 [00:08<00:26, 52623.14 examples/s]Map:  23%|██▎       | 422000/1836912 [00:08<00:26, 53691.67 examples/s]Map:  24%|██▍       | 438000/1836912 [00:08<00:26, 52765.29 examples/s]Map:  23%|██▎       | 428000/1836912 [00:08<00:26, 53752.33 examples/s]Map:  24%|██▍       | 444000/1836912 [00:08<00:26, 53239.88 examples/s]Map:  24%|██▎       | 434000/1836912 [00:08<00:26, 53771.69 examples/s]Map:  24%|██▍       | 450000/1836912 [00:08<00:25, 53542.71 examples/s]Map:  24%|██▍       | 440000/1836912 [00:08<00:25, 53808.37 examples/s]Map:  25%|██▍       | 456000/1836912 [00:09<00:25, 53795.15 examples/s]Map:  24%|██▍       | 446000/1836912 [00:09<00:25, 53611.65 examples/s]Map:  25%|██▌       | 462000/1836912 [00:09<00:25, 53747.49 examples/s]Map:  25%|██▍       | 452000/1836912 [00:09<00:25, 53667.20 examples/s]Map:  25%|██▌       | 468000/1836912 [00:09<00:25, 53894.57 examples/s]Map:  25%|██▍       | 458000/1836912 [00:09<00:25, 53642.96 examples/s]Map:  26%|██▌       | 474000/1836912 [00:09<00:25, 53981.74 examples/s]Map:  25%|██▌       | 464000/1836912 [00:09<00:25, 53719.31 examples/s]Map:  26%|██▌       | 470000/1836912 [00:09<00:25, 53657.49 examples/s]Map:  26%|██▌       | 476000/1836912 [00:09<00:25, 53677.29 examples/s]Map:  26%|██▌       | 482000/1836912 [00:09<00:35, 37940.02 examples/s]Map:  26%|██▌       | 482000/1836912 [00:09<00:25, 53524.32 examples/s]Map:  27%|██▋       | 488000/1836912 [00:09<00:32, 41485.05 examples/s]Map:  27%|██▋       | 494000/1836912 [00:09<00:30, 44471.72 examples/s]Map:  27%|██▋       | 500000/1836912 [00:10<00:28, 46871.64 examples/s]Map:  27%|██▋       | 488000/1836912 [00:10<00:37, 36331.16 examples/s]Map:  28%|██▊       | 506000/1836912 [00:10<00:27, 48587.17 examples/s]Map:  27%|██▋       | 494000/1836912 [00:10<00:33, 40236.74 examples/s]Map:  28%|██▊       | 512000/1836912 [00:10<00:26, 50169.69 examples/s]Map:  27%|██▋       | 500000/1836912 [00:10<00:30, 43570.32 examples/s]Map:  28%|██▊       | 518000/1836912 [00:10<00:25, 51353.75 examples/s]Map:  28%|██▊       | 506000/1836912 [00:10<00:28, 46222.27 examples/s]Map:  29%|██▊       | 524000/1836912 [00:10<00:25, 51865.06 examples/s]Map:  28%|██▊       | 512000/1836912 [00:10<00:27, 48153.91 examples/s]Map:  29%|██▉       | 530000/1836912 [00:10<00:24, 52309.51 examples/s]Map:  28%|██▊       | 518000/1836912 [00:10<00:26, 49672.10 examples/s]Map:  29%|██▉       | 536000/1836912 [00:10<00:24, 52962.03 examples/s]Map:  29%|██▊       | 524000/1836912 [00:10<00:25, 50893.24 examples/s]Map:  30%|██▉       | 542000/1836912 [00:10<00:24, 53350.25 examples/s]Map:  29%|██▉       | 530000/1836912 [00:10<00:25, 51710.42 examples/s]Map:  30%|██▉       | 548000/1836912 [00:10<00:23, 53738.26 examples/s]Map:  29%|██▉       | 536000/1836912 [00:10<00:24, 52335.20 examples/s]Map:  30%|███       | 554000/1836912 [00:11<00:23, 53942.16 examples/s]Map:  30%|██▉       | 542000/1836912 [00:11<00:24, 52746.93 examples/s]Map:  30%|███       | 560000/1836912 [00:11<00:23, 54146.96 examples/s]Map:  30%|██▉       | 548000/1836912 [00:11<00:24, 53106.84 examples/s]Map:  31%|███       | 566000/1836912 [00:11<00:23, 54013.96 examples/s]Map:  30%|███       | 554000/1836912 [00:11<00:24, 53208.39 examples/s]Map:  31%|███       | 572000/1836912 [00:11<00:23, 54133.20 examples/s]Map:  30%|███       | 560000/1836912 [00:11<00:23, 53424.42 examples/s]Map:  31%|███▏      | 578000/1836912 [00:11<00:23, 54173.21 examples/s]Map:  31%|███       | 566000/1836912 [00:11<00:23, 53506.61 examples/s]Map:  32%|███▏      | 584000/1836912 [00:11<00:23, 54273.00 examples/s]Map:  31%|███       | 572000/1836912 [00:11<00:23, 53660.74 examples/s]Map:  32%|███▏      | 590000/1836912 [00:11<00:22, 54324.22 examples/s]Map:  31%|███▏      | 578000/1836912 [00:11<00:23, 53729.97 examples/s]Map:  32%|███▏      | 596000/1836912 [00:11<00:22, 54317.97 examples/s]Map:  32%|███▏      | 584000/1836912 [00:11<00:23, 53703.27 examples/s]Map:  33%|███▎      | 602000/1836912 [00:11<00:22, 54294.34 examples/s]Map:  32%|███▏      | 590000/1836912 [00:11<00:23, 53637.54 examples/s]Map:  33%|███▎      | 608000/1836912 [00:12<00:22, 54349.75 examples/s]Map:  32%|███▏      | 596000/1836912 [00:12<00:23, 53410.10 examples/s]Map:  33%|███▎      | 614000/1836912 [00:12<00:22, 54375.88 examples/s]Map:  33%|███▎      | 602000/1836912 [00:12<00:23, 53423.70 examples/s]Map:  34%|███▍      | 620000/1836912 [00:12<00:22, 54430.12 examples/s]Map:  33%|███▎      | 608000/1836912 [00:12<00:22, 53474.14 examples/s]Map:  34%|███▍      | 626000/1836912 [00:12<00:22, 54320.40 examples/s]Map:  33%|███▎      | 614000/1836912 [00:12<00:22, 53491.76 examples/s]Map:  34%|███▍      | 632000/1836912 [00:12<00:22, 54099.52 examples/s]Map:  35%|███▍      | 638000/1836912 [00:12<00:22, 54091.41 examples/s]Map:  34%|███▍      | 620000/1836912 [00:12<00:32, 36955.80 examples/s]Map:  35%|███▌      | 644000/1836912 [00:12<00:22, 54065.03 examples/s]Map:  34%|███▍      | 626000/1836912 [00:12<00:29, 40528.32 examples/s]Map:  35%|███▌      | 650000/1836912 [00:12<00:21, 54118.47 examples/s]Map:  34%|███▍      | 632000/1836912 [00:12<00:27, 43687.23 examples/s]Map:  36%|███▌      | 656000/1836912 [00:12<00:21, 54153.25 examples/s]Map:  35%|███▍      | 638000/1836912 [00:13<00:25, 46173.67 examples/s]Map:  36%|███▌      | 662000/1836912 [00:13<00:21, 53731.98 examples/s]Map:  35%|███▌      | 644000/1836912 [00:13<00:24, 48113.08 examples/s]Map:  35%|███▌      | 650000/1836912 [00:13<00:23, 49492.05 examples/s]Map:  36%|███▋      | 668000/1836912 [00:13<00:31, 36625.43 examples/s]Map:  36%|███▌      | 656000/1836912 [00:13<00:23, 50555.49 examples/s]Map:  37%|███▋      | 674000/1836912 [00:13<00:28, 40522.88 examples/s]Map:  36%|███▌      | 662000/1836912 [00:13<00:22, 51344.53 examples/s]Map:  37%|███▋      | 680000/1836912 [00:13<00:26, 42986.48 examples/s]Map:  36%|███▋      | 668000/1836912 [00:13<00:22, 51958.10 examples/s]Map:  37%|███▋      | 686000/1836912 [00:13<00:25, 45822.87 examples/s]Map:  37%|███▋      | 674000/1836912 [00:13<00:22, 52361.18 examples/s]Map:  38%|███▊      | 692000/1836912 [00:13<00:23, 47910.51 examples/s]Map:  37%|███▋      | 680000/1836912 [00:13<00:22, 52551.79 examples/s]Map:  38%|███▊      | 698000/1836912 [00:13<00:22, 49583.85 examples/s]Map:  37%|███▋      | 686000/1836912 [00:13<00:21, 52764.59 examples/s]Map:  38%|███▊      | 704000/1836912 [00:13<00:22, 50926.81 examples/s]Map:  38%|███▊      | 692000/1836912 [00:14<00:21, 52995.00 examples/s]Map:  39%|███▊      | 710000/1836912 [00:14<00:21, 51865.53 examples/s]Map:  38%|███▊      | 698000/1836912 [00:14<00:21, 53091.89 examples/s]Map:  39%|███▉      | 716000/1836912 [00:14<00:21, 52622.84 examples/s]Map:  38%|███▊      | 704000/1836912 [00:14<00:21, 53204.19 examples/s]Map:  39%|███▉      | 722000/1836912 [00:14<00:21, 53087.91 examples/s]Map:  39%|███▊      | 710000/1836912 [00:14<00:21, 53274.69 examples/s]Map:  40%|███▉      | 728000/1836912 [00:14<00:20, 53488.48 examples/s]Map:  39%|███▉      | 716000/1836912 [00:14<00:21, 53354.14 examples/s]Map:  40%|███▉      | 734000/1836912 [00:14<00:20, 53610.81 examples/s]Map:  40%|████      | 740000/1836912 [00:14<00:20, 53777.22 examples/s]Map:  41%|████      | 746000/1836912 [00:14<00:20, 53858.34 examples/s]Map:  39%|███▉      | 722000/1836912 [00:14<00:30, 37104.66 examples/s]Map:  41%|████      | 752000/1836912 [00:14<00:20, 54038.81 examples/s]Map:  40%|███▉      | 728000/1836912 [00:14<00:27, 40642.65 examples/s]Map:  41%|████▏     | 758000/1836912 [00:14<00:19, 54100.53 examples/s]Map:  40%|███▉      | 734000/1836912 [00:14<00:25, 43842.13 examples/s]Map:  42%|████▏     | 764000/1836912 [00:15<00:19, 54192.37 examples/s]Map:  40%|████      | 740000/1836912 [00:15<00:23, 46388.01 examples/s]Map:  42%|████▏     | 770000/1836912 [00:15<00:19, 54171.29 examples/s]Map:  41%|████      | 746000/1836912 [00:15<00:22, 48361.54 examples/s]Map:  42%|████▏     | 776000/1836912 [00:15<00:19, 54001.81 examples/s]Map:  41%|████      | 752000/1836912 [00:15<00:21, 49865.58 examples/s]Map:  43%|████▎     | 782000/1836912 [00:15<00:19, 54107.95 examples/s]Map:  41%|████▏     | 758000/1836912 [00:15<00:21, 50930.12 examples/s]Map:  43%|████▎     | 788000/1836912 [00:15<00:19, 54169.65 examples/s]Map:  42%|████▏     | 764000/1836912 [00:15<00:20, 51764.45 examples/s]Map:  43%|████▎     | 794000/1836912 [00:15<00:19, 53913.81 examples/s]Map:  42%|████▏     | 770000/1836912 [00:15<00:20, 52081.28 examples/s]Map:  42%|████▏     | 776000/1836912 [00:15<00:20, 52601.09 examples/s]Map:  43%|████▎     | 782000/1836912 [00:15<00:19, 52855.03 examples/s]Map:  44%|████▎     | 800000/1836912 [00:15<00:27, 37938.69 examples/s]Map:  43%|████▎     | 788000/1836912 [00:16<00:19, 53138.02 examples/s]Map:  44%|████▍     | 806000/1836912 [00:16<00:24, 41644.99 examples/s]Map:  43%|████▎     | 794000/1836912 [00:16<00:19, 53236.41 examples/s]Map:  44%|████▍     | 812000/1836912 [00:16<00:23, 44511.92 examples/s]Map:  44%|████▎     | 800000/1836912 [00:16<00:19, 53411.83 examples/s]Map:  45%|████▍     | 818000/1836912 [00:16<00:21, 47040.01 examples/s]Map:  44%|████▍     | 806000/1836912 [00:16<00:19, 53444.22 examples/s]Map:  45%|████▍     | 824000/1836912 [00:16<00:20, 49037.31 examples/s]Map:  44%|████▍     | 812000/1836912 [00:16<00:19, 53531.57 examples/s]Map:  45%|████▌     | 830000/1836912 [00:16<00:19, 50486.61 examples/s]Map:  46%|████▌     | 836000/1836912 [00:16<00:19, 51456.13 examples/s]Map:  46%|████▌     | 842000/1836912 [00:16<00:19, 51776.28 examples/s]Map:  45%|████▍     | 820000/1836912 [00:16<00:26, 38246.33 examples/s]Map:  46%|████▌     | 848000/1836912 [00:16<00:18, 52135.44 examples/s]Map:  45%|████▍     | 826000/1836912 [00:16<00:24, 41555.54 examples/s]Map:  46%|████▋     | 854000/1836912 [00:16<00:18, 52754.42 examples/s]Map:  45%|████▌     | 832000/1836912 [00:16<00:22, 44427.58 examples/s]Map:  47%|████▋     | 860000/1836912 [00:17<00:18, 53187.29 examples/s]Map:  46%|████▌     | 838000/1836912 [00:17<00:21, 46713.53 examples/s]Map:  47%|████▋     | 866000/1836912 [00:17<00:18, 53268.85 examples/s]Map:  46%|████▌     | 844000/1836912 [00:17<00:20, 48492.63 examples/s]Map:  47%|████▋     | 872000/1836912 [00:17<00:18, 53535.08 examples/s]Map:  46%|████▋     | 850000/1836912 [00:17<00:19, 49867.38 examples/s]Map:  48%|████▊     | 878000/1836912 [00:17<00:17, 53686.38 examples/s]Map:  47%|████▋     | 856000/1836912 [00:17<00:19, 50969.35 examples/s]Map:  48%|████▊     | 884000/1836912 [00:17<00:17, 53812.68 examples/s]Map:  47%|████▋     | 862000/1836912 [00:17<00:18, 51712.78 examples/s]Map:  48%|████▊     | 890000/1836912 [00:17<00:17, 53853.10 examples/s]Map:  47%|████▋     | 868000/1836912 [00:17<00:18, 52321.85 examples/s]Map:  49%|████▉     | 896000/1836912 [00:17<00:17, 53984.31 examples/s]Map:  48%|████▊     | 874000/1836912 [00:17<00:18, 52735.02 examples/s]Map:  48%|████▊     | 880000/1836912 [00:17<00:18, 52852.30 examples/s]Map:  49%|████▉     | 902000/1836912 [00:17<00:24, 37934.16 examples/s]Map:  48%|████▊     | 886000/1836912 [00:17<00:17, 53055.97 examples/s]Map:  49%|████▉     | 908000/1836912 [00:18<00:22, 41734.41 examples/s]Map:  49%|████▊     | 892000/1836912 [00:18<00:17, 53291.04 examples/s]Map:  50%|████▉     | 914000/1836912 [00:18<00:20, 44390.28 examples/s]Map:  49%|████▉     | 898000/1836912 [00:18<00:17, 53374.40 examples/s]Map:  50%|█████     | 920000/1836912 [00:18<00:19, 46656.31 examples/s]Map:  49%|████▉     | 904000/1836912 [00:18<00:17, 53464.25 examples/s]Map:  50%|█████     | 926000/1836912 [00:18<00:18, 48590.70 examples/s]Map:  50%|████▉     | 910000/1836912 [00:18<00:17, 53543.00 examples/s]Map:  51%|█████     | 932000/1836912 [00:18<00:18, 50233.92 examples/s]Map:  50%|████▉     | 916000/1836912 [00:18<00:17, 53643.89 examples/s]Map:  51%|█████     | 938000/1836912 [00:18<00:17, 51195.96 examples/s]Map:  50%|█████     | 922000/1836912 [00:18<00:17, 53615.87 examples/s]Map:  51%|█████▏    | 944000/1836912 [00:18<00:17, 51914.57 examples/s]Map:  51%|█████     | 928000/1836912 [00:18<00:16, 53572.68 examples/s]Map:  52%|█████▏    | 950000/1836912 [00:18<00:16, 52580.66 examples/s]Map:  51%|█████     | 934000/1836912 [00:18<00:16, 53545.98 examples/s]Map:  52%|█████▏    | 956000/1836912 [00:18<00:16, 53132.70 examples/s]Map:  51%|█████     | 940000/1836912 [00:19<00:16, 53550.85 examples/s]Map:  52%|█████▏    | 962000/1836912 [00:19<00:16, 53339.05 examples/s]Map:  51%|█████▏    | 946000/1836912 [00:19<00:16, 53396.09 examples/s]Map:  53%|█████▎    | 968000/1836912 [00:19<00:16, 53366.97 examples/s]Map:  52%|█████▏    | 952000/1836912 [00:19<00:16, 53443.87 examples/s]Map:  53%|█████▎    | 974000/1836912 [00:19<00:16, 53647.40 examples/s]Map:  52%|█████▏    | 958000/1836912 [00:19<00:16, 53377.30 examples/s]Map:  53%|█████▎    | 980000/1836912 [00:19<00:15, 53898.58 examples/s]Map:  52%|█████▏    | 964000/1836912 [00:19<00:16, 53426.06 examples/s]Map:  54%|█████▎    | 986000/1836912 [00:19<00:15, 53996.89 examples/s]Map:  53%|█████▎    | 970000/1836912 [00:19<00:16, 53421.85 examples/s]Map:  54%|█████▍    | 992000/1836912 [00:19<00:15, 53795.59 examples/s]Map:  53%|█████▎    | 976000/1836912 [00:19<00:16, 53481.64 examples/s]Map:  53%|█████▎    | 982000/1836912 [00:19<00:16, 53376.85 examples/s]Map:  54%|█████▍    | 988000/1836912 [00:19<00:15, 53417.18 examples/s]Map:  54%|█████▍    | 1000000/1836912 [00:19<00:21, 39733.75 examples/s]Map:  54%|█████▍    | 994000/1836912 [00:20<00:15, 53430.13 examples/s]Map:  55%|█████▍    | 1006000/1836912 [00:20<00:19, 42885.28 examples/s]Map:  54%|█████▍    | 1000000/1836912 [00:20<00:15, 53456.49 examples/s]Map:  55%|█████▌    | 1011000/1836912 [00:20<00:19, 42793.82 examples/s]Map:  55%|█████▌    | 1017000/1836912 [00:20<00:17, 45709.45 examples/s]Map:  56%|█████▌    | 1023000/1836912 [00:20<00:16, 47975.44 examples/s]Map:  55%|█████▍    | 1006000/1836912 [00:20<00:22, 37001.42 examples/s]Map:  56%|█████▌    | 1029000/1836912 [00:20<00:16, 49717.49 examples/s]Map:  55%|█████▌    | 1012000/1836912 [00:20<00:20, 40758.08 examples/s]Map:  56%|█████▋    | 1035000/1836912 [00:20<00:15, 51022.00 examples/s]Map:  55%|█████▌    | 1018000/1836912 [00:20<00:18, 43895.66 examples/s]Map:  57%|█████▋    | 1041000/1836912 [00:20<00:15, 51832.95 examples/s]Map:  56%|█████▌    | 1024000/1836912 [00:20<00:17, 46419.01 examples/s]Map:  57%|█████▋    | 1047000/1836912 [00:20<00:15, 52455.82 examples/s]Map:  56%|█████▌    | 1030000/1836912 [00:20<00:16, 48315.42 examples/s]Map:  57%|█████▋    | 1053000/1836912 [00:20<00:14, 52972.87 examples/s]Map:  56%|█████▋    | 1036000/1836912 [00:20<00:16, 49841.58 examples/s]Map:  58%|█████▊    | 1059000/1836912 [00:21<00:14, 53431.14 examples/s]Map:  57%|█████▋    | 1042000/1836912 [00:21<00:15, 50838.14 examples/s]Map:  58%|█████▊    | 1065000/1836912 [00:21<00:14, 53701.93 examples/s]Map:  57%|█████▋    | 1048000/1836912 [00:21<00:15, 51637.31 examples/s]Map:  58%|█████▊    | 1071000/1836912 [00:21<00:14, 53943.10 examples/s]Map:  57%|█████▋    | 1054000/1836912 [00:21<00:14, 52217.32 examples/s]Map:  59%|█████▊    | 1077000/1836912 [00:21<00:14, 54011.88 examples/s]Map:  58%|█████▊    | 1060000/1836912 [00:21<00:14, 52629.55 examples/s]Map:  59%|█████▉    | 1083000/1836912 [00:21<00:13, 53918.95 examples/s]Map:  58%|█████▊    | 1066000/1836912 [00:21<00:14, 52547.69 examples/s]Map:  59%|█████▉    | 1089000/1836912 [00:21<00:13, 53995.39 examples/s]Map:  58%|█████▊    | 1072000/1836912 [00:21<00:14, 52795.70 examples/s]Map:  60%|█████▉    | 1095000/1836912 [00:21<00:13, 54161.70 examples/s]Map:  59%|█████▊    | 1078000/1836912 [00:21<00:14, 52922.98 examples/s]Map:  60%|█████▉    | 1101000/1836912 [00:21<00:13, 54201.02 examples/s]Map:  59%|█████▉    | 1084000/1836912 [00:21<00:14, 53107.66 examples/s]Map:  60%|██████    | 1107000/1836912 [00:21<00:13, 54317.66 examples/s]Map:  59%|█████▉    | 1090000/1836912 [00:21<00:14, 53201.67 examples/s]Map:  61%|██████    | 1113000/1836912 [00:22<00:13, 54260.96 examples/s]Map:  60%|█████▉    | 1096000/1836912 [00:22<00:13, 53367.76 examples/s]Map:  61%|██████    | 1119000/1836912 [00:22<00:13, 54342.35 examples/s]Map:  60%|█████▉    | 1102000/1836912 [00:22<00:13, 53405.97 examples/s]Map:  61%|██████    | 1125000/1836912 [00:22<00:13, 54364.46 examples/s]Map:  60%|██████    | 1108000/1836912 [00:22<00:13, 53480.04 examples/s]Map:  62%|██████▏   | 1131000/1836912 [00:22<00:12, 54444.36 examples/s]Map:  61%|██████    | 1114000/1836912 [00:22<00:13, 53489.58 examples/s]Map:  62%|██████▏   | 1137000/1836912 [00:22<00:12, 54385.98 examples/s]Map:  61%|██████    | 1120000/1836912 [00:22<00:13, 53573.17 examples/s]Map:  62%|██████▏   | 1143000/1836912 [00:22<00:12, 54276.80 examples/s]Map:  61%|██████▏   | 1126000/1836912 [00:22<00:13, 53571.07 examples/s]Map:  63%|██████▎   | 1149000/1836912 [00:22<00:12, 53925.24 examples/s]Map:  62%|██████▏   | 1132000/1836912 [00:22<00:13, 53630.26 examples/s]Map:  63%|██████▎   | 1155000/1836912 [00:22<00:12, 54060.07 examples/s]Map:  63%|██████▎   | 1161000/1836912 [00:22<00:12, 54147.83 examples/s]Map:  62%|██████▏   | 1138000/1836912 [00:23<00:18, 38168.57 examples/s]Map:  64%|██████▎   | 1167000/1836912 [00:23<00:12, 54232.84 examples/s]Map:  62%|██████▏   | 1144000/1836912 [00:23<00:16, 41779.45 examples/s]Map:  64%|██████▍   | 1173000/1836912 [00:23<00:12, 54257.92 examples/s]Map:  63%|██████▎   | 1150000/1836912 [00:23<00:15, 44582.22 examples/s]Map:  64%|██████▍   | 1179000/1836912 [00:23<00:12, 54328.74 examples/s]Map:  63%|██████▎   | 1156000/1836912 [00:23<00:14, 46977.46 examples/s]Map:  65%|██████▍   | 1185000/1836912 [00:23<00:11, 54331.87 examples/s]Map:  63%|██████▎   | 1162000/1836912 [00:23<00:13, 48722.97 examples/s]Map:  64%|██████▎   | 1168000/1836912 [00:23<00:13, 50035.44 examples/s]Map:  65%|██████▍   | 1192000/1836912 [00:23<00:16, 38153.47 examples/s]Map:  64%|██████▍   | 1174000/1836912 [00:23<00:12, 51002.94 examples/s]Map:  65%|██████▌   | 1198000/1836912 [00:23<00:15, 41724.77 examples/s]Map:  64%|██████▍   | 1180000/1836912 [00:23<00:12, 51773.79 examples/s]Map:  66%|██████▌   | 1204000/1836912 [00:23<00:14, 44773.09 examples/s]Map:  65%|██████▍   | 1186000/1836912 [00:23<00:12, 52306.48 examples/s]Map:  66%|██████▌   | 1210000/1836912 [00:24<00:13, 47171.16 examples/s]Map:  65%|██████▍   | 1192000/1836912 [00:24<00:12, 52716.33 examples/s]Map:  66%|██████▌   | 1216000/1836912 [00:24<00:12, 48905.50 examples/s]Map:  65%|██████▌   | 1198000/1836912 [00:24<00:12, 52950.36 examples/s]Map:  67%|██████▋   | 1222000/1836912 [00:24<00:12, 50362.60 examples/s]Map:  66%|██████▌   | 1204000/1836912 [00:24<00:11, 53146.78 examples/s]Map:  67%|██████▋   | 1228000/1836912 [00:24<00:11, 51514.90 examples/s]Map:  66%|██████▌   | 1210000/1836912 [00:24<00:11, 52933.54 examples/s]Map:  67%|██████▋   | 1234000/1836912 [00:24<00:11, 52308.78 examples/s]Map:  66%|██████▋   | 1218000/1836912 [00:24<00:12, 51304.46 examples/s]Map:  68%|██████▊   | 1240000/1836912 [00:24<00:11, 51284.41 examples/s]Map:  67%|██████▋   | 1224000/1836912 [00:24<00:11, 51939.16 examples/s]Map:  68%|██████▊   | 1247000/1836912 [00:24<00:12, 47052.33 examples/s]Map:  67%|██████▋   | 1232000/1836912 [00:24<00:12, 49679.83 examples/s]Map:  68%|██████▊   | 1253000/1836912 [00:24<00:12, 47653.88 examples/s]Map:  68%|██████▊   | 1258000/1836912 [00:24<00:12, 46217.98 examples/s]Map:  69%|██████▉   | 1264000/1836912 [00:25<00:11, 48400.36 examples/s]Map:  68%|██████▊   | 1240000/1836912 [00:25<00:15, 38420.51 examples/s]Map:  69%|██████▉   | 1270000/1836912 [00:25<00:11, 50035.29 examples/s]Map:  68%|██████▊   | 1246000/1836912 [00:25<00:14, 41478.36 examples/s]Map:  69%|██████▉   | 1276000/1836912 [00:25<00:10, 51274.13 examples/s]Map:  68%|██████▊   | 1252000/1836912 [00:25<00:13, 44203.36 examples/s]Map:  70%|██████▉   | 1282000/1836912 [00:25<00:10, 52043.69 examples/s]Map:  68%|██████▊   | 1258000/1836912 [00:25<00:12, 46461.80 examples/s]Map:  70%|███████   | 1288000/1836912 [00:25<00:10, 52409.83 examples/s]Map:  69%|██████▉   | 1264000/1836912 [00:25<00:11, 48297.03 examples/s]Map:  70%|███████   | 1294000/1836912 [00:25<00:10, 52931.25 examples/s]Map:  69%|██████▉   | 1270000/1836912 [00:25<00:11, 49528.90 examples/s]Map:  71%|███████   | 1300000/1836912 [00:25<00:10, 53357.87 examples/s]Map:  69%|██████▉   | 1276000/1836912 [00:25<00:11, 50664.62 examples/s]Map:  71%|███████   | 1306000/1836912 [00:25<00:09, 53652.91 examples/s]Map:  70%|██████▉   | 1282000/1836912 [00:25<00:10, 51468.77 examples/s]Map:  71%|███████▏  | 1312000/1836912 [00:25<00:09, 53872.50 examples/s]Map:  70%|███████   | 1288000/1836912 [00:26<00:10, 51918.05 examples/s]Map:  70%|███████   | 1294000/1836912 [00:26<00:10, 52314.23 examples/s]Map:  72%|███████▏  | 1318000/1836912 [00:26<00:13, 37517.48 examples/s]Map:  71%|███████   | 1300000/1836912 [00:26<00:10, 52613.29 examples/s]Map:  72%|███████▏  | 1324000/1836912 [00:26<00:12, 41300.74 examples/s]Map:  71%|███████   | 1306000/1836912 [00:26<00:10, 52268.63 examples/s]Map:  72%|███████▏  | 1330000/1836912 [00:26<00:11, 44457.24 examples/s]Map:  71%|███████▏  | 1312000/1836912 [00:26<00:09, 52643.87 examples/s]Map:  73%|███████▎  | 1336000/1836912 [00:26<00:10, 47011.57 examples/s]Map:  72%|███████▏  | 1318000/1836912 [00:26<00:09, 52900.18 examples/s]Map:  73%|███████▎  | 1342000/1836912 [00:26<00:10, 48945.41 examples/s]Map:  72%|███████▏  | 1324000/1836912 [00:26<00:09, 53090.68 examples/s]Map:  73%|███████▎  | 1348000/1836912 [00:26<00:09, 50429.97 examples/s]Map:  72%|███████▏  | 1330000/1836912 [00:26<00:09, 53241.42 examples/s]Map:  74%|███████▎  | 1354000/1836912 [00:26<00:09, 51515.55 examples/s]Map:  74%|███████▍  | 1360000/1836912 [00:27<00:09, 52338.60 examples/s]Map:  73%|███████▎  | 1338000/1836912 [00:27<00:12, 39557.41 examples/s]Map:  74%|███████▍  | 1366000/1836912 [00:27<00:08, 52870.08 examples/s]Map:  73%|███████▎  | 1344000/1836912 [00:27<00:11, 42706.19 examples/s]Map:  75%|███████▍  | 1372000/1836912 [00:27<00:08, 53131.95 examples/s]Map:  73%|███████▎  | 1350000/1836912 [00:27<00:10, 45305.27 examples/s]Map:  75%|███████▌  | 1378000/1836912 [00:27<00:08, 53269.53 examples/s]Map:  74%|███████▍  | 1356000/1836912 [00:27<00:10, 47488.31 examples/s]Map:  75%|███████▌  | 1384000/1836912 [00:27<00:08, 53506.64 examples/s]Map:  74%|███████▍  | 1362000/1836912 [00:27<00:09, 49119.60 examples/s]Map:  76%|███████▌  | 1390000/1836912 [00:27<00:08, 53691.81 examples/s]Map:  74%|███████▍  | 1368000/1836912 [00:27<00:09, 50015.95 examples/s]Map:  76%|███████▌  | 1396000/1836912 [00:27<00:08, 52854.71 examples/s]Map:  75%|███████▍  | 1374000/1836912 [00:27<00:09, 51008.10 examples/s]Map:  76%|███████▋  | 1402000/1836912 [00:27<00:08, 53221.00 examples/s]Map:  75%|███████▌  | 1380000/1836912 [00:27<00:08, 51855.58 examples/s]Map:  77%|███████▋  | 1408000/1836912 [00:27<00:08, 53576.52 examples/s]Map:  75%|███████▌  | 1386000/1836912 [00:28<00:08, 52055.99 examples/s]Map:  77%|███████▋  | 1414000/1836912 [00:28<00:07, 53758.96 examples/s]Map:  76%|███████▌  | 1392000/1836912 [00:28<00:08, 52506.40 examples/s]Map:  76%|███████▌  | 1398000/1836912 [00:28<00:08, 52762.23 examples/s]Map:  77%|███████▋  | 1420000/1836912 [00:28<00:10, 38888.33 examples/s]Map:  76%|███████▋  | 1404000/1836912 [00:28<00:08, 53056.13 examples/s]Map:  78%|███████▊  | 1426000/1836912 [00:28<00:09, 42496.17 examples/s]Map:  77%|███████▋  | 1410000/1836912 [00:28<00:08, 53230.91 examples/s]Map:  78%|███████▊  | 1432000/1836912 [00:28<00:08, 45489.26 examples/s]Map:  77%|███████▋  | 1416000/1836912 [00:28<00:07, 53420.73 examples/s]Map:  78%|███████▊  | 1438000/1836912 [00:28<00:08, 47778.14 examples/s]Map:  77%|███████▋  | 1422000/1836912 [00:28<00:07, 53478.49 examples/s]Map:  79%|███████▊  | 1444000/1836912 [00:28<00:07, 49568.91 examples/s]Map:  78%|███████▊  | 1428000/1836912 [00:28<00:07, 53605.57 examples/s]Map:  79%|███████▉  | 1450000/1836912 [00:28<00:07, 50859.54 examples/s]Map:  78%|███████▊  | 1434000/1836912 [00:28<00:07, 53618.31 examples/s]Map:  79%|███████▉  | 1456000/1836912 [00:28<00:07, 51854.02 examples/s]Map:  78%|███████▊  | 1440000/1836912 [00:29<00:07, 53686.18 examples/s]Map:  80%|███████▉  | 1462000/1836912 [00:29<00:07, 52566.63 examples/s]Map:  79%|███████▊  | 1446000/1836912 [00:29<00:07, 53662.35 examples/s]Map:  80%|███████▉  | 1468000/1836912 [00:29<00:06, 53127.31 examples/s]Map:  79%|███████▉  | 1452000/1836912 [00:29<00:07, 53602.56 examples/s]Map:  80%|████████  | 1474000/1836912 [00:29<00:06, 53456.12 examples/s]Map:  79%|███████▉  | 1458000/1836912 [00:29<00:07, 53623.67 examples/s]Map:  81%|████████  | 1480000/1836912 [00:29<00:06, 53745.19 examples/s]Map:  80%|███████▉  | 1464000/1836912 [00:31<00:42, 8682.78 examples/s] Map:  81%|████████  | 1487000/1836912 [00:31<00:38, 9052.22 examples/s] Map:  80%|████████  | 1470000/1836912 [00:31<00:31, 11598.25 examples/s]Map:  81%|████████▏ | 1493000/1836912 [00:31<00:28, 11932.58 examples/s]Map:  80%|████████  | 1476000/1836912 [00:31<00:23, 15170.01 examples/s]Map:  82%|████████▏ | 1499000/1836912 [00:31<00:21, 15468.69 examples/s]Map:  81%|████████  | 1482000/1836912 [00:31<00:18, 19325.68 examples/s]Map:  82%|████████▏ | 1505000/1836912 [00:31<00:16, 19591.84 examples/s]Map:  81%|████████  | 1488000/1836912 [00:31<00:14, 23885.26 examples/s]Map:  82%|████████▏ | 1511000/1836912 [00:31<00:13, 24148.81 examples/s]Map:  81%|████████▏ | 1494000/1836912 [00:31<00:11, 28622.59 examples/s]Map:  82%|████████▏ | 1500000/1836912 [00:32<00:10, 33243.69 examples/s]Map:  83%|████████▎ | 1518000/1836912 [00:32<00:12, 24851.42 examples/s]Map:  82%|████████▏ | 1506000/1836912 [00:32<00:08, 37512.06 examples/s]Map:  83%|████████▎ | 1524000/1836912 [00:32<00:10, 29428.97 examples/s]Map:  82%|████████▏ | 1512000/1836912 [00:32<00:07, 41211.28 examples/s]Map:  83%|████████▎ | 1530000/1836912 [00:32<00:09, 33941.24 examples/s]Map:  83%|████████▎ | 1518000/1836912 [00:32<00:07, 44284.44 examples/s]Map:  84%|████████▎ | 1536000/1836912 [00:32<00:07, 38126.91 examples/s]Map:  84%|████████▍ | 1542000/1836912 [00:32<00:07, 41589.22 examples/s]Map:  83%|████████▎ | 1524000/1836912 [00:32<00:09, 33774.68 examples/s]Map:  84%|████████▍ | 1548000/1836912 [00:32<00:06, 44604.79 examples/s]Map:  83%|████████▎ | 1530000/1836912 [00:32<00:08, 37882.98 examples/s]Map:  85%|████████▍ | 1554000/1836912 [00:32<00:06, 46954.91 examples/s]Map:  84%|████████▎ | 1536000/1836912 [00:32<00:07, 41541.99 examples/s]Map:  85%|████████▍ | 1560000/1836912 [00:32<00:05, 48839.76 examples/s]Map:  84%|████████▍ | 1542000/1836912 [00:33<00:06, 44532.82 examples/s]Map:  85%|████████▌ | 1566000/1836912 [00:33<00:05, 50343.15 examples/s]Map:  84%|████████▍ | 1548000/1836912 [00:33<00:06, 46934.25 examples/s]Map:  86%|████████▌ | 1572000/1836912 [00:33<00:05, 51522.90 examples/s]Map:  85%|████████▍ | 1554000/1836912 [00:33<00:05, 48722.75 examples/s]Map:  86%|████████▌ | 1578000/1836912 [00:33<00:04, 52263.14 examples/s]Map:  85%|████████▍ | 1560000/1836912 [00:33<00:05, 50095.13 examples/s]Map:  86%|████████▌ | 1584000/1836912 [00:33<00:04, 52920.83 examples/s]Map:  85%|████████▌ | 1566000/1836912 [00:33<00:05, 51079.87 examples/s]Map:  87%|████████▋ | 1590000/1836912 [00:33<00:04, 53316.27 examples/s]Map:  86%|████████▌ | 1572000/1836912 [00:33<00:05, 51825.99 examples/s]Map:  87%|████████▋ | 1596000/1836912 [00:33<00:04, 53647.34 examples/s]Map:  86%|████████▌ | 1578000/1836912 [00:33<00:04, 52362.49 examples/s]Map:  87%|████████▋ | 1602000/1836912 [00:33<00:04, 53857.41 examples/s]Map:  86%|████████▌ | 1584000/1836912 [00:33<00:04, 52702.58 examples/s]Map:  88%|████████▊ | 1608000/1836912 [00:33<00:04, 54043.22 examples/s]Map:  87%|████████▋ | 1590000/1836912 [00:33<00:04, 52947.03 examples/s]Map:  88%|████████▊ | 1614000/1836912 [00:33<00:04, 54172.22 examples/s]Map:  87%|████████▋ | 1596000/1836912 [00:34<00:04, 53189.18 examples/s]Map:  88%|████████▊ | 1620000/1836912 [00:34<00:04, 53673.31 examples/s]Map:  87%|████████▋ | 1602000/1836912 [00:34<00:04, 53318.49 examples/s]Map:  89%|████████▊ | 1626000/1836912 [00:34<00:03, 53863.65 examples/s]Map:  88%|████████▊ | 1608000/1836912 [00:34<00:04, 53414.11 examples/s]Map:  89%|████████▉ | 1632000/1836912 [00:34<00:03, 54034.01 examples/s]Map:  88%|████████▊ | 1614000/1836912 [00:34<00:04, 53463.19 examples/s]Map:  89%|████████▉ | 1638000/1836912 [00:34<00:03, 53766.02 examples/s]Map:  88%|████████▊ | 1620000/1836912 [00:34<00:04, 53585.63 examples/s]Map:  89%|████████▉ | 1644000/1836912 [00:34<00:03, 53955.41 examples/s]Map:  89%|████████▊ | 1626000/1836912 [00:34<00:03, 53620.62 examples/s]Map:  90%|████████▉ | 1650000/1836912 [00:34<00:03, 53717.53 examples/s]Map:  89%|████████▉ | 1632000/1836912 [00:34<00:03, 53682.53 examples/s]Map:  90%|█████████ | 1656000/1836912 [00:34<00:03, 53771.43 examples/s]Map:  89%|████████▉ | 1638000/1836912 [00:34<00:03, 53609.93 examples/s]Map:  90%|█████████ | 1662000/1836912 [00:34<00:03, 53753.13 examples/s]Map:  89%|████████▉ | 1644000/1836912 [00:34<00:03, 53643.27 examples/s]Map:  91%|█████████ | 1668000/1836912 [00:34<00:03, 53906.57 examples/s]Map:  90%|████████▉ | 1650000/1836912 [00:35<00:03, 53604.50 examples/s]Map:  91%|█████████ | 1674000/1836912 [00:35<00:03, 53958.63 examples/s]Map:  91%|█████████▏| 1680000/1836912 [00:35<00:02, 54017.60 examples/s]Map:  92%|█████████▏| 1686000/1836912 [00:35<00:02, 53807.94 examples/s]Map:  90%|█████████ | 1656000/1836912 [00:35<00:04, 39090.16 examples/s]Map:  92%|█████████▏| 1692000/1836912 [00:35<00:02, 53959.02 examples/s]Map:  90%|█████████ | 1662000/1836912 [00:35<00:04, 42495.57 examples/s]Map:  92%|█████████▏| 1698000/1836912 [00:35<00:02, 54021.67 examples/s]Map:  91%|█████████ | 1668000/1836912 [00:35<00:03, 45315.24 examples/s]Map:  91%|█████████ | 1674000/1836912 [00:35<00:03, 47320.25 examples/s]Map:  91%|█████████▏| 1680000/1836912 [00:35<00:03, 49019.73 examples/s]Map:  93%|█████████▎| 1704000/1836912 [00:35<00:03, 37643.94 examples/s]Map:  92%|█████████▏| 1686000/1836912 [00:35<00:03, 50246.86 examples/s]Map:  93%|█████████▎| 1710000/1836912 [00:35<00:03, 41415.41 examples/s]Map:  92%|█████████▏| 1692000/1836912 [00:35<00:02, 51202.27 examples/s]Map:  93%|█████████▎| 1716000/1836912 [00:35<00:02, 44623.32 examples/s]Map:  92%|█████████▏| 1698000/1836912 [00:36<00:02, 51847.26 examples/s]Map:  94%|█████████▎| 1722000/1836912 [00:36<00:02, 47133.08 examples/s]Map:  93%|█████████▎| 1704000/1836912 [00:36<00:02, 52373.24 examples/s]Map:  94%|█████████▍| 1728000/1836912 [00:36<00:02, 49137.69 examples/s]Map:  93%|█████████▎| 1710000/1836912 [00:36<00:02, 52671.55 examples/s]Map:  94%|█████████▍| 1734000/1836912 [00:36<00:02, 50584.02 examples/s]Map:  93%|█████████▎| 1716000/1836912 [00:36<00:02, 52949.31 examples/s]Map:  95%|█████████▍| 1740000/1836912 [00:36<00:01, 51661.62 examples/s]Map:  94%|█████████▎| 1722000/1836912 [00:36<00:02, 53082.35 examples/s]Map:  95%|█████████▌| 1746000/1836912 [00:36<00:01, 52200.62 examples/s]Map:  94%|█████████▍| 1728000/1836912 [00:36<00:02, 53215.98 examples/s]Map:  95%|█████████▌| 1752000/1836912 [00:36<00:01, 52786.17 examples/s]Map:  94%|█████████▍| 1734000/1836912 [00:36<00:01, 53021.69 examples/s]Map:  96%|█████████▌| 1758000/1836912 [00:36<00:01, 53201.33 examples/s]Map:  95%|█████████▍| 1740000/1836912 [00:36<00:01, 52007.50 examples/s]Map:  96%|█████████▌| 1764000/1836912 [00:36<00:01, 53575.46 examples/s]Map:  95%|█████████▌| 1746000/1836912 [00:36<00:01, 52403.78 examples/s]Map:  96%|█████████▋| 1770000/1836912 [00:36<00:01, 53805.35 examples/s]Map:  95%|█████████▌| 1752000/1836912 [00:37<00:01, 52763.93 examples/s]Map:  97%|█████████▋| 1776000/1836912 [00:37<00:01, 54000.48 examples/s]Map:  97%|█████████▋| 1782000/1836912 [00:37<00:01, 53747.00 examples/s]Map:  97%|█████████▋| 1788000/1836912 [00:37<00:00, 53708.13 examples/s]Map:  96%|█████████▌| 1758000/1836912 [00:37<00:02, 38012.19 examples/s]Map:  98%|█████████▊| 1794000/1836912 [00:37<00:00, 53841.93 examples/s]Map:  96%|█████████▌| 1764000/1836912 [00:37<00:01, 41614.47 examples/s]Map:  98%|█████████▊| 1800000/1836912 [00:37<00:00, 53736.04 examples/s]Map:  96%|█████████▋| 1770000/1836912 [00:37<00:01, 44312.03 examples/s]Map:  98%|█████████▊| 1806000/1836912 [00:37<00:00, 53870.45 examples/s]Map:  97%|█████████▋| 1776000/1836912 [00:37<00:01, 46701.77 examples/s]Map:  99%|█████████▊| 1812000/1836912 [00:37<00:00, 54031.40 examples/s]Map:  97%|█████████▋| 1782000/1836912 [00:37<00:01, 48546.86 examples/s]Map:  99%|█████████▉| 1818000/1836912 [00:37<00:00, 53355.85 examples/s]Map:  97%|█████████▋| 1788000/1836912 [00:37<00:00, 49801.13 examples/s]Map:  99%|█████████▉| 1824000/1836912 [00:37<00:00, 53497.54 examples/s]Map:  98%|█████████▊| 1794000/1836912 [00:38<00:00, 50806.27 examples/s]Map: 100%|█████████▉| 1830000/1836912 [00:38<00:00, 53524.88 examples/s]Map:  98%|█████████▊| 1800000/1836912 [00:38<00:00, 51610.87 examples/s]Map:  98%|█████████▊| 1806000/1836912 [00:38<00:00, 52149.19 examples/s]Map: 100%|█████████▉| 1836000/1836912 [00:38<00:00, 38491.46 examples/s]Map:  99%|█████████▊| 1812000/1836912 [00:38<00:00, 52482.29 examples/s]Map:  99%|█████████▉| 1818000/1836912 [00:38<00:00, 52780.83 examples/s]Map:  99%|█████████▉| 1824000/1836912 [00:38<00:00, 53000.35 examples/s]Map: 100%|█████████▉| 1830000/1836912 [00:38<00:00, 53132.70 examples/s]Map: 100%|█████████▉| 1836000/1836912 [00:38<00:00, 53176.19 examples/s]Map: 100%|██████████| 1836912/1836912 [00:45<00:00, 39944.83 examples/s]
Map: 100%|██████████| 1836912/1836912 [00:46<00:00, 39846.62 examples/s]
[INFO:swift] train_dataset: Dataset({
    features: ['messages', 'images'],
    num_rows: 1818543
})
[INFO:swift] val_dataset: Dataset({
    features: ['messages', 'images'],
    num_rows: 18369
})
[INFO:swift] [INPUT_IDS] [3961, 678, 562, 6901, 14135, 98, 144, 144, 8308, 10427, 59601, 59568, -200, 144, 144, 59653, 959, 568, 59651, 18772, 687, 59568, 78, 12689, 98, 144, 8308, 20921, 59601, 58404, 80, 86, 80, 5122, 81, 79, 83, 5122, 81, 84, 77, 5122, 81, 86, 81, 38260, 662, 10811, 1288, 59604, 144, 8308]
[INFO:swift] [INPUT] You are a helpful assistant.

### Human: [-200 * 1]

[refer] Locate 1 aircraft.
### Assistant:{<393><426><470><494>} (bottom right)
###
[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 58404, 80, 86, 80, 5122, 81, 79, 83, 5122, 81, 84, 77, 5122, 81, 86, 81, 38260, 662, 10811, 1288, 59604, 144, 8308]
[INFO:swift] [LABELS] [-100 * 29]{<393><426><470><494>} (bottom right)
###
[INFO:swift] model: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlavaLlamaForCausalLM(
      (model): LlavaLlamaModel(
        (embed_tokens): Embedding(64000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=512, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=512, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=512, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=512, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
        (vision_tower): CLIPVisionTower(
          (vision_tower): CLIPVisionModel(
            (vision_model): CLIPVisionTransformer(
              (embeddings): CLIPVisionEmbeddings(
                (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)
                (position_embedding): Embedding(1025, 1280)
              )
              (pre_layrnorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (encoder): CLIPEncoder(
                (layers): ModuleList(
                  (0-31): 32 x CLIPEncoderLayer(
                    (self_attn): CLIPSdpaAttention(
                      (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
                      (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
                      (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
                      (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
                    )
                    (layer_norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (mlp): CLIPMLP(
                      (activation_fn): GELUActivation()
                      (fc1): Linear(in_features=1280, out_features=5120, bias=True)
                      (fc2): Linear(in_features=5120, out_features=1280, bias=True)
                    )
                    (layer_norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (post_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (mm_projector): Sequential(
          (0): Linear(in_features=1280, out_features=4096, bias=True)
          (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
          (2): GELU(approximate='none')
          (3): Linear(in_features=4096, out_features=4096, bias=True)
          (4): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=4096, out_features=64000, bias=False)
    )
  )
)
[INFO:swift] model_parameter_info: PeftModelForCausalLM: 6732.9828M Params (18.1535M Trainable [0.2696%]), 0.0011M Buffers.
[WARNING:modelscope] Authentication has expired, please re-login for uploading or accessing controlled entities.
[WARNING:modelscope] Authentication has expired, please re-login for uploading or accessing controlled entities.
/root/.conda/envs/swift-new-4/lib/python3.10/site-packages/swift/trainers/mixin.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
/root/.conda/envs/swift-new-4/lib/python3.10/site-packages/swift/trainers/mixin.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[INFO:swift] The logging file will be saved in: /root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v5-20250205-094115/logging.jsonl
[INFO:swift] Successfully registered post_encode hook: ['PeftModelForCausalLM']
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Currently logged in as: mazhiming312 (mzmgo) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.5
wandb: Run data is saved locally in /root/code/sarchat-exp/wandb/run-20250205_094422-4b9pjlxt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /root/code/sarchat-exp/swift_output/SARChat-Yi-VL-6B-Lora/v5-20250205-094115
wandb: ⭐️ View project at https://wandb.ai/mzmgo/huggingface
wandb: 🚀 View run at https://wandb.ai/mzmgo/huggingface/runs/4b9pjlxt
Train:   0%|          | 0/56829 [00:00<?, ?it/s][rank0]:[W205 09:44:41.185992479 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W205 09:44:41.224863088 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:  18%|█▊        | 10001/56829 [00:15<01:10, 661.42it/s]Train:  18%|█▊        | 10003/56829 [00:29<01:10, 661.42it/s]Train:  18%|█▊        | 10004/56829 [00:33<03:15, 239.64it/s]Train:  18%|█▊        | 10005/56829 [00:40<04:17, 181.61it/s]                                                             {'loss': 0.18675467, 'grad_norm': 0.50077313, 'learning_rate': 9.825e-05, 'memory(GiB)': 34.16, 'train_speed(iter/s)': 184.678333, 'epoch': 0.18, 'global_step/max_steps': '10005/56829', 'percentage': '17.61%', 'elapsed_time': '40s', 'remaining_time': '3m 8s'}
Train:  18%|█▊        | 10005/56829 [00:40<04:17, 181.61it/s]Train:  18%|█▊        | 10005/56829 [00:40<04:17, 181.61it/s]Train:  18%|█▊        | 10006/56829 [00:46<05:41, 137.04it/s]Train:  18%|█▊        | 10007/56829 [00:52<07:42, 101.28it/s]Train:  18%|█▊        | 10008/56829 [00:59<10:47, 72.27it/s] Train:  18%|█▊        | 10009/56829 [01:05<14:51, 52.55it/s]Train:  18%|█▊        | 10010/56829 [01:11<20:41, 37.72it/s]                                                            {'loss': 0.19913111, 'grad_norm': 0.42460483, 'learning_rate': 9.824e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 116.995819, 'epoch': 0.18, 'global_step/max_steps': '10010/56829', 'percentage': '17.61%', 'elapsed_time': '1m 11s', 'remaining_time': '5m 35s'}
Train:  18%|█▊        | 10010/56829 [01:11<20:41, 37.72it/s]Train:  18%|█▊        | 10010/56829 [01:11<20:41, 37.72it/s]Train:  18%|█▊        | 10011/56829 [01:17<28:57, 26.95it/s]Train:  18%|█▊        | 10012/56829 [01:23<40:39, 19.19it/s]Train:  18%|█▊        | 10013/56829 [01:30<57:29, 13.57it/s]Train:  18%|█▊        | 10014/56829 [01:36<1:20:53,  9.65it/s]Train:  18%|█▊        | 10015/56829 [01:42<1:54:13,  6.83it/s]                                                              {'loss': 0.13651061, 'grad_norm': 0.48647463, 'learning_rate': 9.824e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 86.076316, 'epoch': 0.18, 'global_step/max_steps': '10015/56829', 'percentage': '17.62%', 'elapsed_time': '1m 42s', 'remaining_time': '7m 59s'}
Train:  18%|█▊        | 10015/56829 [01:42<1:54:13,  6.83it/s]Train:  18%|█▊        | 10015/56829 [01:42<1:54:13,  6.83it/s]Train:  18%|█▊        | 10016/56829 [01:48<2:40:48,  4.85it/s]Train:  18%|█▊        | 10017/56829 [01:55<3:50:03,  3.39it/s]Train:  18%|█▊        | 10018/56829 [02:01<5:19:06,  2.44it/s]Train:  18%|█▊        | 10019/56829 [02:07<7:20:33,  1.77it/s]Train:  18%|█▊        | 10020/56829 [02:13<10:17:46,  1.26it/s]                                                               {'loss': 0.28793101, 'grad_norm': 0.33312449, 'learning_rate': 9.824e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 67.788554, 'epoch': 0.18, 'global_step/max_steps': '10020/56829', 'percentage': '17.63%', 'elapsed_time': '2m 13s', 'remaining_time': '10m 25s'}
Train:  18%|█▊        | 10020/56829 [02:13<10:17:46,  1.26it/s]Train:  18%|█▊        | 10020/56829 [02:13<10:17:46,  1.26it/s]Train:  18%|█▊        | 10021/56829 [02:20<13:53:48,  1.07s/it]Train:  18%|█▊        | 10022/56829 [02:26<18:22:31,  1.41s/it]Train:  18%|█▊        | 10023/56829 [02:32<23:45:05,  1.83s/it]Train:  18%|█▊        | 10024/56829 [02:38<29:58:34,  2.31s/it]Train:  18%|█▊        | 10025/56829 [02:44<37:03:33,  2.85s/it]                                                               {'loss': 0.17629496, 'grad_norm': 0.76477569, 'learning_rate': 9.823e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 56.124389, 'epoch': 0.18, 'global_step/max_steps': '10025/56829', 'percentage': '17.64%', 'elapsed_time': '2m 44s', 'remaining_time': '12m 49s'}
Train:  18%|█▊        | 10025/56829 [02:44<37:03:33,  2.85s/it]Train:  18%|█▊        | 10025/56829 [02:44<37:03:33,  2.85s/it]Train:  18%|█▊        | 10026/56829 [02:50<44:19:22,  3.41s/it]Train:  18%|█▊        | 10027/56829 [02:57<51:35:14,  3.97s/it]Train:  18%|█▊        | 10028/56829 [03:03<58:14:03,  4.48s/it]Train:  18%|█▊        | 10029/56829 [03:09<63:36:35,  4.89s/it]Train:  18%|█▊        | 10030/56829 [03:16<67:45:11,  5.21s/it]                                                               {'loss': 0.32820807, 'grad_norm': 0.34058806, 'learning_rate': 9.823e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 47.766959, 'epoch': 0.18, 'global_step/max_steps': '10030/56829', 'percentage': '17.65%', 'elapsed_time': '3m 16s', 'remaining_time': '15m 15s'}
Train:  18%|█▊        | 10030/56829 [03:16<67:45:11,  5.21s/it]Train:  18%|█▊        | 10030/56829 [03:16<67:45:11,  5.21s/it]Train:  18%|█▊        | 10031/56829 [03:22<70:55:11,  5.46s/it]Train:  18%|█▊        | 10032/56829 [03:28<73:33:21,  5.66s/it]Train:  18%|█▊        | 10033/56829 [03:34<75:30:23,  5.81s/it]Train:  18%|█▊        | 10034/56829 [03:40<77:17:15,  5.95s/it]Train:  18%|█▊        | 10035/56829 [03:47<78:12:49,  6.02s/it]                                                               {'loss': 0.22877309, 'grad_norm': 0.66108483, 'learning_rate': 9.822e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 41.642188, 'epoch': 0.18, 'global_step/max_steps': '10035/56829', 'percentage': '17.66%', 'elapsed_time': '3m 47s', 'remaining_time': '17m 39s'}
Train:  18%|█▊        | 10035/56829 [03:47<78:12:49,  6.02s/it]Train:  18%|█▊        | 10035/56829 [03:47<78:12:49,  6.02s/it]Train:  18%|█▊        | 10036/56829 [03:53<78:54:48,  6.07s/it]Train:  18%|█▊        | 10037/56829 [03:59<78:55:57,  6.07s/it]Train:  18%|█▊        | 10038/56829 [04:05<79:34:41,  6.12s/it]Train:  18%|█▊        | 10039/56829 [04:11<79:43:34,  6.13s/it]Train:  18%|█▊        | 10040/56829 [04:17<79:32:03,  6.12s/it]                                                               {'loss': 0.19721706, 'grad_norm': 0.58397645, 'learning_rate': 9.822e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 36.946517, 'epoch': 0.18, 'global_step/max_steps': '10040/56829', 'percentage': '17.67%', 'elapsed_time': '4m 17s', 'remaining_time': '20m 1s'}
Train:  18%|█▊        | 10040/56829 [04:17<79:32:03,  6.12s/it]Train:  18%|█▊        | 10040/56829 [04:17<79:32:03,  6.12s/it]Train:  18%|█▊        | 10041/56829 [04:23<79:26:07,  6.11s/it]Train:  18%|█▊        | 10042/56829 [04:30<80:18:02,  6.18s/it]Train:  18%|█▊        | 10043/56829 [04:36<80:01:31,  6.16s/it]Train:  18%|█▊        | 10044/56829 [04:42<80:00:50,  6.16s/it]Train:  18%|█▊        | 10045/56829 [04:49<81:17:54,  6.26s/it]                                                               {'loss': 0.18659285, 'grad_norm': 0.41951323, 'learning_rate': 9.822e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 33.159867, 'epoch': 0.18, 'global_step/max_steps': '10045/56829', 'percentage': '17.68%', 'elapsed_time': '4m 49s', 'remaining_time': '22m 26s'}
Train:  18%|█▊        | 10045/56829 [04:49<81:17:54,  6.26s/it]Train:  18%|█▊        | 10045/56829 [04:49<81:17:54,  6.26s/it]Train:  18%|█▊        | 10046/56829 [04:55<81:13:47,  6.25s/it]Train:  18%|█▊        | 10047/56829 [05:01<80:44:33,  6.21s/it]Train:  18%|█▊        | 10048/56829 [05:07<80:20:49,  6.18s/it]Train:  18%|█▊        | 10049/56829 [05:13<80:07:46,  6.17s/it]Train:  18%|█▊        | 10050/56829 [05:19<80:28:36,  6.19s/it]                                                               {'loss': 0.16425519, 'grad_norm': 0.37671059, 'learning_rate': 9.821e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 30.109227, 'epoch': 0.18, 'global_step/max_steps': '10050/56829', 'percentage': '17.68%', 'elapsed_time': '5m 19s', 'remaining_time': '24m 49s'}
Train:  18%|█▊        | 10050/56829 [05:19<80:28:36,  6.19s/it]Train:  18%|█▊        | 10050/56829 [05:19<80:28:36,  6.19s/it]Train:  18%|█▊        | 10051/56829 [05:26<80:24:22,  6.19s/it]Train:  18%|█▊        | 10052/56829 [05:32<80:10:25,  6.17s/it]Train:  18%|█▊        | 10053/56829 [05:38<80:11:04,  6.17s/it]Train:  18%|█▊        | 10054/56829 [05:44<80:40:47,  6.21s/it]Train:  18%|█▊        | 10055/56829 [05:50<81:00:11,  6.23s/it]                                                               {'loss': 0.26802602, 'grad_norm': 0.57933366, 'learning_rate': 9.821e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 27.558732, 'epoch': 0.18, 'global_step/max_steps': '10055/56829', 'percentage': '17.69%', 'elapsed_time': '5m 50s', 'remaining_time': '27m 12s'}
Train:  18%|█▊        | 10055/56829 [05:51<81:00:11,  6.23s/it]Train:  18%|█▊        | 10055/56829 [05:51<81:00:11,  6.23s/it]Train:  18%|█▊        | 10056/56829 [05:57<80:50:41,  6.22s/it]Train:  18%|█▊        | 10057/56829 [06:03<80:27:46,  6.19s/it]Train:  18%|█▊        | 10058/56829 [06:09<80:22:24,  6.19s/it]Train:  18%|█▊        | 10059/56829 [06:15<80:28:17,  6.19s/it]Train:  18%|█▊        | 10060/56829 [06:21<80:09:13,  6.17s/it]                                                               {'loss': 0.16427214, 'grad_norm': 0.36005676, 'learning_rate': 9.82e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 25.425276, 'epoch': 0.18, 'global_step/max_steps': '10060/56829', 'percentage': '17.70%', 'elapsed_time': '6m 21s', 'remaining_time': '29m 35s'}
Train:  18%|█▊        | 10060/56829 [06:21<80:09:13,  6.17s/it]Train:  18%|█▊        | 10060/56829 [06:21<80:09:13,  6.17s/it]Train:  18%|█▊        | 10061/56829 [06:28<81:40:53,  6.29s/it]Train:  18%|█▊        | 10062/56829 [06:34<81:15:34,  6.26s/it]Train:  18%|█▊        | 10063/56829 [06:40<81:52:48,  6.30s/it]Train:  18%|█▊        | 10064/56829 [06:47<81:16:57,  6.26s/it]Train:  18%|█▊        | 10065/56829 [06:53<80:42:10,  6.21s/it]                                                               {'loss': 0.25247707, 'grad_norm': 0.41733721, 'learning_rate': 9.82e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 23.566481, 'epoch': 0.18, 'global_step/max_steps': '10065/56829', 'percentage': '17.71%', 'elapsed_time': '6m 53s', 'remaining_time': '31m 59s'}
Train:  18%|█▊        | 10065/56829 [06:53<80:42:10,  6.21s/it]Train:  18%|█▊        | 10065/56829 [06:53<80:42:10,  6.21s/it]Train:  18%|█▊        | 10066/56829 [06:59<80:52:15,  6.23s/it]Train:  18%|█▊        | 10067/56829 [07:05<81:35:56,  6.28s/it]Train:  18%|█▊        | 10068/56829 [07:11<80:54:00,  6.23s/it]Train:  18%|█▊        | 10069/56829 [07:18<80:23:30,  6.19s/it]Train:  18%|█▊        | 10070/56829 [07:24<80:41:02,  6.21s/it]                                                               {'loss': 0.23454027, 'grad_norm': 0.38135973, 'learning_rate': 9.82e-05, 'memory(GiB)': 41.45, 'train_speed(iter/s)': 21.976142, 'epoch': 0.18, 'global_step/max_steps': '10070/56829', 'percentage': '17.72%', 'elapsed_time': '7m 24s', 'remaining_time': '34m 23s'}
Train:  18%|█▊        | 10070/56829 [07:24<80:41:02,  6.21s/it]Train:  18%|█▊        | 10070/56829 [07:24<80:41:02,  6.21s/it]Train:  18%|█▊        | 10071/56829 [07:30<82:21:14,  6.34s/it]Train:  18%|█▊        | 10072/56829 [07:37<81:27:11,  6.27s/it]Train:  18%|█▊        | 10073/56829 [07:43<81:58:25,  6.31s/it]Train:  18%|█▊        | 10074/56829 [07:49<81:16:25,  6.26s/it]Train:  18%|█▊        | 10075/56829 [07:55<81:19:55,  6.26s/it]                                                               {'loss': 0.24012232, 'grad_norm': 0.25291771, 'learning_rate': 9.819e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 20.570195, 'epoch': 0.18, 'global_step/max_steps': '10075/56829', 'percentage': '17.73%', 'elapsed_time': '7m 55s', 'remaining_time': '36m 48s'}
Train:  18%|█▊        | 10075/56829 [07:55<81:19:55,  6.26s/it]Train:  18%|█▊        | 10075/56829 [07:55<81:19:55,  6.26s/it]Train:  18%|█▊        | 10076/56829 [08:02<80:56:17,  6.23s/it]Train:  18%|█▊        | 10077/56829 [08:08<80:25:17,  6.19s/it]Train:  18%|█▊        | 10078/56829 [08:14<80:06:14,  6.17s/it]Train:  18%|█▊        | 10079/56829 [08:20<80:20:05,  6.19s/it]Train:  18%|█▊        | 10080/56829 [08:26<80:03:56,  6.17s/it]                                                               {'loss': 0.22945094, 'grad_norm': 0.37852734, 'learning_rate': 9.819e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 19.365838, 'epoch': 0.18, 'global_step/max_steps': '10080/56829', 'percentage': '17.74%', 'elapsed_time': '8m 26s', 'remaining_time': '39m 9s'}
Train:  18%|█▊        | 10080/56829 [08:26<80:03:56,  6.17s/it]Train:  18%|█▊        | 10080/56829 [08:26<80:03:56,  6.17s/it]Train:  18%|█▊        | 10081/56829 [08:32<80:09:47,  6.17s/it]Train:  18%|█▊        | 10082/56829 [08:39<80:15:42,  6.18s/it]Train:  18%|█▊        | 10083/56829 [08:45<81:04:05,  6.24s/it]Train:  18%|█▊        | 10084/56829 [08:51<81:12:02,  6.25s/it]Train:  18%|█▊        | 10085/56829 [08:57<80:29:54,  6.20s/it]                                                               {'loss': 0.23947034, 'grad_norm': 0.27732661, 'learning_rate': 9.818e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 18.282146, 'epoch': 0.18, 'global_step/max_steps': '10085/56829', 'percentage': '17.75%', 'elapsed_time': '8m 57s', 'remaining_time': '41m 32s'}
Train:  18%|█▊        | 10085/56829 [08:57<80:29:54,  6.20s/it]Train:  18%|█▊        | 10085/56829 [08:57<80:29:54,  6.20s/it]Train:  18%|█▊        | 10086/56829 [09:03<80:17:48,  6.18s/it]Train:  18%|█▊        | 10087/56829 [09:10<81:05:52,  6.25s/it]Train:  18%|█▊        | 10088/56829 [09:16<80:55:40,  6.23s/it]Train:  18%|█▊        | 10089/56829 [09:22<80:40:00,  6.21s/it]Train:  18%|█▊        | 10090/56829 [09:28<80:18:20,  6.19s/it]                                                               {'loss': 0.20544412, 'grad_norm': 0.36639348, 'learning_rate': 9.818e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 17.317039, 'epoch': 0.18, 'global_step/max_steps': '10090/56829', 'percentage': '17.76%', 'elapsed_time': '9m 28s', 'remaining_time': '43m 54s'}
Train:  18%|█▊        | 10090/56829 [09:28<80:18:20,  6.19s/it]Train:  18%|█▊        | 10090/56829 [09:28<80:18:20,  6.19s/it]Train:  18%|█▊        | 10091/56829 [09:35<81:07:21,  6.25s/it]Train:  18%|█▊        | 10092/56829 [09:41<80:46:44,  6.22s/it]Train:  18%|█▊        | 10093/56829 [09:47<80:24:52,  6.19s/it]Train:  18%|█▊        | 10094/56829 [09:53<80:10:17,  6.18s/it]Train:  18%|█▊        | 10095/56829 [09:59<80:41:23,  6.22s/it]                                                               {'loss': 0.20150902, 'grad_norm': 0.31954306, 'learning_rate': 9.818e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 16.447023, 'epoch': 0.18, 'global_step/max_steps': '10095/56829', 'percentage': '17.76%', 'elapsed_time': '9m 59s', 'remaining_time': '46m 17s'}
Train:  18%|█▊        | 10095/56829 [09:59<80:41:23,  6.22s/it]Train:  18%|█▊        | 10095/56829 [09:59<80:41:23,  6.22s/it]Train:  18%|█▊        | 10096/56829 [10:06<80:39:10,  6.21s/it]Train:  18%|█▊        | 10097/56829 [10:12<80:34:01,  6.21s/it]Train:  18%|█▊        | 10098/56829 [10:18<80:07:09,  6.17s/it]Train:  18%|█▊        | 10099/56829 [10:24<80:00:10,  6.16s/it]Train:  18%|█▊        | 10100/56829 [10:30<81:01:35,  6.24s/it]                                                               {'loss': 0.20895672, 'grad_norm': 0.44160756, 'learning_rate': 9.817e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 15.662607, 'epoch': 0.18, 'global_step/max_steps': '10100/56829', 'percentage': '17.77%', 'elapsed_time': '10m 30s', 'remaining_time': '48m 39s'}
Train:  18%|█▊        | 10100/56829 [10:30<81:01:35,  6.24s/it]Train:  18%|█▊        | 10100/56829 [10:30<81:01:35,  6.24s/it]Train:  18%|█▊        | 10101/56829 [10:37<80:52:49,  6.23s/it]Train:  18%|█▊        | 10102/56829 [10:43<80:35:15,  6.21s/it]Train:  18%|█▊        | 10103/56829 [10:49<80:25:42,  6.20s/it]Train:  18%|█▊        | 10104/56829 [10:55<80:29:02,  6.20s/it]Train:  18%|█▊        | 10105/56829 [11:01<80:05:53,  6.17s/it]                                                               {'loss': 0.19331366, 'grad_norm': 0.33712539, 'learning_rate': 9.817e-05, 'memory(GiB)': 45.58, 'train_speed(iter/s)': 14.955021, 'epoch': 0.18, 'global_step/max_steps': '10105/56829', 'percentage': '17.78%', 'elapsed_time': '11m 1s', 'remaining_time': '51m 0s'}
Train:  18%|█▊        | 10105/56829 [11:01<80:05:53,  6.17s/it]Train:  18%|█▊        | 10105/56829 [11:01<80:05:53,  6.17s/it]Train:  18%|█▊        | 10106/56829 [11:07<80:06:32,  6.17s/it]Train:  18%|█▊        | 10107/56829 [11:14<79:54:35,  6.16s/it]Train:  18%|█▊        | 10108/56829 [11:20<81:29:49,  6.28s/it]Train:  18%|█▊        | 10109/56829 [11:27<84:59:35,  6.55s/it]Train:  18%|█▊        | 10110/56829 [11:33<83:17:46,  6.42s/it]                                                               {'loss': 0.25992966, 'grad_norm': 0.76562607, 'learning_rate': 9.816e-05, 'memory(GiB)': 50.97, 'train_speed(iter/s)': 14.282768, 'epoch': 0.18, 'global_step/max_steps': '10110/56829', 'percentage': '17.79%', 'elapsed_time': '11m 33s', 'remaining_time': '53m 26s'}
Train:  18%|█▊        | 10110/56829 [11:33<83:17:46,  6.42s/it]Train:  18%|█▊        | 10110/56829 [11:33<83:17:46,  6.42s/it]Train:  18%|█▊        | 10111/56829 [11:40<82:16:27,  6.34s/it]Train:  18%|█▊        | 10112/56829 [11:46<81:49:43,  6.31s/it]Train:  18%|█▊        | 10113/56829 [11:52<81:04:13,  6.25s/it]Train:  18%|█▊        | 10114/56829 [11:58<80:44:07,  6.22s/it]Train:  18%|█▊        | 10115/56829 [12:04<80:13:04,  6.18s/it]                                                               {'loss': 0.25001502, 'grad_norm': 0.35925466, 'learning_rate': 9.816e-05, 'memory(GiB)': 50.97, 'train_speed(iter/s)': 13.695052, 'epoch': 0.18, 'global_step/max_steps': '10115/56829', 'percentage': '17.80%', 'elapsed_time': '12m 4s', 'remaining_time': '55m 46s'}
Train:  18%|█▊        | 10115/56829 [12:04<80:13:04,  6.18s/it]Train:  18%|█▊        | 10115/56829 [12:04<80:13:04,  6.18s/it]Train:  18%|█▊        | 10116/56829 [12:11<81:49:17,  6.31s/it]Train:  18%|█▊        | 10117/56829 [12:17<81:03:54,  6.25s/it]Train:  18%|█▊        | 10118/56829 [12:23<80:35:15,  6.21s/it]Train:  18%|█▊        | 10119/56829 [12:29<80:10:51,  6.18s/it]Train:  18%|█▊        | 10120/56829 [12:35<80:17:19,  6.19s/it]                                                               {'loss': 0.18820877, 'grad_norm': 0.23186381, 'learning_rate': 9.815e-05, 'memory(GiB)': 50.97, 'train_speed(iter/s)': 13.147344, 'epoch': 0.18, 'global_step/max_steps': '10120/56829', 'percentage': '17.81%', 'elapsed_time': '12m 35s', 'remaining_time': '58m 8s'}
Train:  18%|█▊        | 10120/56829 [12:35<80:17:19,  6.19s/it]Train:  18%|█▊        | 10120/56829 [12:35<80:17:19,  6.19s/it]Train:  18%|█▊        | 10121/56829 [12:42<80:10:52,  6.18s/it]Train:  18%|█▊        | 10122/56829 [12:48<80:02:21,  6.17s/it]